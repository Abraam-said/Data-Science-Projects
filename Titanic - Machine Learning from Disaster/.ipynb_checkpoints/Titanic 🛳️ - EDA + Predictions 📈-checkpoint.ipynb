{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "277cca4d",
   "metadata": {},
   "source": [
    "<div style=\" text-align: center; justify-content: center; align-items: center; background-color:#272727; padding: 10px;  border-radius: 10px;\">\n",
    "    <h1 style=\"color: white;  margin-top: 9px; margin-bottom: 9px; \"><center>\n",
    "        Titanic üõ≥Ô∏è | EDA  + Predictions üìà\n",
    "</center></h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41752ee3",
   "metadata": {},
   "source": [
    "<center>\n",
    "        <img src=\"https://t3.ftcdn.net/jpg/05/79/15/62/360_F_579156262_spzSiXdm3G8535tU6qRqMzvFLy9WwaZz.jpg\" width=500>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c458408d",
   "metadata": {},
   "source": [
    "# Introduction üìñ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8385947f",
   "metadata": {},
   "source": [
    "The Titanic dataset, a historical dataset derived from the tragic sinking of the RMS Titanic in 1912, has become a quintessential benchmark in the field of data analysis and machine learning. This dataset provides a glimpse into the diverse demographics of the passengers aboard the Titanic, including information such as age, gender, class, and fare. The dataset also records whether each passenger survived the disaster or not.\n",
    "\n",
    "In this analysis, the goal is to predict passenger survival on the Titanic by employing data exploration and advanced techniques. The focus is on building a model that anticipates whether a passenger survived the disaster. Through the examination of individual traits, patterns will be uncovered, illuminating the crucial factors influencing survival."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9657379e",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "# Table of Contents üìë"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d727d5e2",
   "metadata": {},
   "source": [
    "> - [1 - Import Libraries üìö](#1)\n",
    "> - [2 - Data Exploration üîé](#2)\n",
    "> - [3 - Exploratory Data Analysis üìä](#3)\n",
    ">    - [3.1- Univariate Analysis](#3.1)\n",
    ">    - [3.2- Bivariate Analysis](#3.2)\n",
    "> - [4 - Data Preprocessing ‚öíÔ∏è](#4)\n",
    ">    - [4.1- Feature Extraction](#4.1)\n",
    ">    - [4.2- Handling Missing Data](#4.2)\n",
    ">    - [4.3- Handling Categorical Data](#4.3)\n",
    ">    - [4.4- Data Split to Train and Test Sets](#4.4)\n",
    ">    - [4.5- Handling Imbalanced Data](#4.5)\n",
    ">    - [4.6- Feature Scaling](#4.6)\n",
    "> - [5- Models Training and Evaluation ‚öôÔ∏è](#5)\n",
    ">    - [5.1- K-fold Cross-Validation Evaluation and Feature Selection](#5.1)\n",
    "> - [6- Hyperparameter Tuning üõ†Ô∏è](#6)\n",
    ">    - [6.1- ROC Curve for Final Model (LGBMClassifier)](#6.1)\n",
    "> - [7- Submissionüí°](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc35a9a",
   "metadata": {},
   "source": [
    "---------------------------------------\n",
    "<a class=\"anchor\"  id=\"1\"></a>\n",
    "# 1- Import Libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a454823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Data Preprocessing Libraries\n",
    "from datasist.structdata import detect_outliers\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from category_encoders import BinaryEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Machine Learing (classification models) Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, SelectKBest, f_regression, RFE, SelectFromModel\n",
    "from imblearn.pipeline import Pipeline \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report, roc_curve, roc_auc_score \n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941116c4",
   "metadata": {},
   "source": [
    "-------------------\n",
    "<a class=\"anchor\"  id=\"2\"></a>\n",
    "# 2- Data Exploration üîé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58b949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Train csv file\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e458a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dataset shape\n",
    "print(\"Number of Columns in Train data\",df_train.shape[1])\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Number of Rows in Train data\",df_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbdde7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Test csv file\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_test.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0385974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dataset shape\n",
    "print(\"Number of Columns in Train data\",df_test.shape[1])\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Number of Rows in Train data\",df_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f588b5eb",
   "metadata": {},
   "source": [
    "### Feature Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7599beb4",
   "metadata": {},
   "source": [
    "> - **`survival`**:\tSurvival (0 = No, 1 = Yes)\n",
    "> - **`pclass`**: Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd)\n",
    "> - **`sex`**: Sex\t\n",
    "> - **`Age`**: Age in years\t\n",
    "> - **`sibsp`**: Number of siblings / spouses aboard the Titanic\t\n",
    "> - **`parch`**: Number of parents / children aboard the Titanic\t\n",
    "> - **`ticket`**: Ticket number\t\n",
    "> - **`fare`**:\tPassenger fare\t\n",
    "> - **`cabin`**: Cabin number\t\n",
    "> - **`embarked`**:\tPort of Embarkation\t(C = Cherbourg, Q = Queenstown, S = Southampton)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9082abc",
   "metadata": {},
   "source": [
    "## Working On Train.CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad19656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8237f1f5",
   "metadata": {},
   "source": [
    "- Need to drop PassengerId and Ticket columns as they are unique identifier and not useful for predictions.\n",
    "- Age, Cabin and Embarked columns has missing values and needs to be handled   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d87be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping PassengerId and Ticket columns as they are unique identifier and not useful for predictions.\n",
    "df_train = df_train.drop(['PassengerId', 'Ticket'], axis=1)\n",
    "\n",
    "df_test = df_test.drop('Ticket', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7adf155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking count the number of unique values in each column of the data\n",
    "df_train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b649385",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052a0227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive analysis for categorical data\n",
    "df_train.describe(include='O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fc2889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive analysis for numerical data\n",
    "df_train.describe().style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21785dd3",
   "metadata": {},
   "source": [
    "> I noticed the lowest age recorded is 0.42, likely indicating a baby's age in years. Which looks fine, so we can use it in our analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9377f0",
   "metadata": {},
   "source": [
    "-----------------------------------------\n",
    "<a class=\"anchor\"  id=\"3\"></a>\n",
    "# 3- Exploratory Data Analysis üìä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b724ad",
   "metadata": {},
   "source": [
    "<a class=\"anchor\"  id=\"3.1\"></a>\n",
    "## 3.1- Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222e282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "survived_counts = df_train['Survived'].value_counts()\n",
    "plt.pie(survived_counts, labels=['Not Survived', 'Survived'], autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Proportion of Passengers Who Survived')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635dc1e1",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- A higher percentage of passengers (61.6%) did not survive, whereas a relatively smaller percentage (38.4%) survived the incident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12afccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df_train, x='Age', kde=True)\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb40d00",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- The age distribution appears to be right-skewed, indicating that there are relatively more younger passengers in the dataset. However, there are no apparent outliers in the age data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35599407",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df_train, x='Fare', kde=True)\n",
    "plt.title('Fare Distribution')\n",
    "plt.xlabel('Fare')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5fa0d3",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- The fare distribution exhibits a significant right skewness, primarily because of the presence of outliers in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307ebd32",
   "metadata": {},
   "source": [
    "---------------------------------------------\n",
    "<a class=\"anchor\"  id=\"3.2\"></a>\n",
    "## 3.2- Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89fa3d4",
   "metadata": {},
   "source": [
    "### Analyzing the Impact of Sex on Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b4668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df_train, x='Sex', color='Survived', barmode='group', title='Survival by Sex', text_auto=True,\n",
    "                  color_discrete_sequence=['#ff7f0e', '#3498db'],)\n",
    "\n",
    "fig.update_traces(marker=dict(line=dict(width=2, color='DarkSlateGrey')))\n",
    "\n",
    "# format the layout\n",
    "fig.update_layout(\n",
    "    xaxis=dict(showgrid=False, zeroline=False),\n",
    "    yaxis=dict(zeroline=False, gridcolor='white'),\n",
    "    paper_bgcolor='rgb(233,233,233)',\n",
    "    plot_bgcolor='rgb(233,233,233)',\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dac9961",
   "metadata": {},
   "outputs": [],
   "source": [
    "women = df_train.loc[df_train.Sex == 'female'][\"Survived\"]\n",
    "rate_women = sum(women)/len(women) * 100\n",
    "\n",
    "print(\"% of women who survived:\", rate_women)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedc8365",
   "metadata": {},
   "outputs": [],
   "source": [
    "men = df_train.loc[df_train.Sex == 'male'][\"Survived\"]\n",
    "rate_men = sum(men)/len(men) * 100\n",
    "\n",
    "print(\"% of men who survived:\", rate_men)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7fb9cf",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- More women, about 74%, survived compared to men, with only around 19% of men surviving. This shows that there were big differences in who survived based on gender."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dfb462",
   "metadata": {},
   "source": [
    "### Analyzing the Impact of Pclass on Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4597a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df_train, x='Pclass', color='Survived', barmode='group', title='Survival by Pclass', text_auto=True,\n",
    "                  color_discrete_sequence=['#ff7f0e', '#3498db'],)\n",
    "\n",
    "fig.update_traces(marker=dict(line=dict(width=2, color='DarkSlateGrey')))\n",
    "\n",
    "# format the layout\n",
    "fig.update_layout(\n",
    "    xaxis=dict(showgrid=False, zeroline=False),\n",
    "    yaxis=dict(zeroline=False, gridcolor='white'),\n",
    "    paper_bgcolor='rgb(233,233,233)',\n",
    "    plot_bgcolor='rgb(233,233,233)',\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9249773",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- The data indicates that first-class passengers had the highest survival probability, second-class passengers had roughly equal chances of survival and non-survival, while third-class passengers had the lowest probability of surviving. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5508114",
   "metadata": {},
   "source": [
    "### Analyzing the Impact of Age on Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ec1f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df_train, x='Age', color='Survived', title='Survival by Age',\n",
    "                   marginal='box', barmode='group', color_discrete_sequence=['#ff7f0e', '#3498db'],\n",
    "                   )\n",
    "\n",
    "fig.update_traces(marker=dict(line=dict(width=2, color='DarkSlateGrey')))\n",
    "\n",
    "# format the layout\n",
    "fig.update_layout(\n",
    "    xaxis=dict(showgrid=False, zeroline=False),\n",
    "    yaxis=dict(zeroline=False, gridcolor='white'),\n",
    "    paper_bgcolor='rgb(233,233,233)',\n",
    "    plot_bgcolor='rgb(233,233,233)',\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c567b2",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- Age has a small effect on survival. As people get older, they are a little less likely to survive. However, age is not the most important factor, and there are other things that matter more for survival."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf15415",
   "metadata": {},
   "source": [
    "### Analyzing the Impact of SibSp on Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831bbaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df_train, x='SibSp', y='Survived', palette='Set2')\n",
    "plt.title('Survival Rate by SibSp')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eb7a02",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- Passengers with larger numbers of siblings/spouses have lower survival rates, while those with no or few siblings/spouses (1 or 2) have higher chances of surviving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390c9f5b",
   "metadata": {},
   "source": [
    "### Analyzing the Impact of Parch on Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dde9f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df_train, x='Parch', y='Survived', palette='Set2')\n",
    "plt.title('Survival Rate by Parch')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eed5a9",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- Passengers with larger numbers of Parents/Children have lower survival rates, while those with no or few Parents/Children (1 or 2) have higher chances of surviving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdb35aa",
   "metadata": {},
   "source": [
    "### Analyzing the Impact of Fare Cost on Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54a11a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(df_train, x='Survived', y='Fare', color='Survived', \n",
    "              title='Fare Distribution by Survival', \n",
    "              labels={\"Fare\": \"Passenger Fare\", \"Survived\": \"Survived\"},\n",
    "              color_discrete_sequence=['#ff7f0e', '#3498db'])\n",
    "\n",
    "# format the layout\n",
    "fig.update_layout(\n",
    "    xaxis=dict(showgrid=False, zeroline=False),\n",
    "    yaxis=dict(zeroline=False, gridcolor='white'),\n",
    "    paper_bgcolor='rgb(233,233,233)',\n",
    "    plot_bgcolor='rgb(233,233,233)',\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadf324e",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- Based on the Boxplot, it seems that when the fare cost goes up, there is only a small increase in the chance of surviving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bbdbbf",
   "metadata": {},
   "source": [
    "### Analyzing the Impact of Embarkation Point on Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac6d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df_train, x='Embarked', y='Survived', palette='Set2')\n",
    "plt.title('Survival Rate by Embarkation Point')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f95036",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- People who boarded the Titanic in Cherbourg, France, had the highest chance of surviving. Those who boarded from Southampton, England, and Queenstown (Cobh), Ireland, had lower chances of survival."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6335c355",
   "metadata": {},
   "source": [
    "------------------\n",
    "<a class=\"anchor\"  id=\"4\"></a>\n",
    "# 4- Data Preprocessing ‚öíÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caa58e2",
   "metadata": {},
   "source": [
    "<a class=\"anchor\"  id=\"4.1\"></a>\n",
    "## 4.1- Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ef5225",
   "metadata": {},
   "source": [
    "Creating a **``Title Names``** column provides a more detailed categorization of individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f014e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['title_name'] = df_train['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n",
    "\n",
    "df_test['title_name'] = df_test['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b163fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['title_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100b5703",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['title_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e6a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_titles(title):\n",
    "    if title in ['Mr', 'Mrs', 'Miss', 'Master']:\n",
    "        return title\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Applying the function to df_train title_name column\n",
    "df_train['title_name'] = df_train['title_name'].apply(categorize_titles)\n",
    "\n",
    "df_test['title_name'] = df_test['title_name'].apply(categorize_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f955d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the 'Name' column as full names aren't needed for building the model\n",
    "df_train = df_train.drop('Name', axis=1)\n",
    "\n",
    "df_test = df_test.drop('Name', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5752731",
   "metadata": {},
   "source": [
    "Creating a **``Family Size``** column by adding the 'SibSp' (number of siblings/spouses aboard) and 'Parch' (number of parents/children aboard) columns provides a comprehensive measure of the family size for each passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d4f097",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['family_size'] = df_train['SibSp'] + df_train['Parch']\n",
    "\n",
    "df_test['family_size'] = df_test['SibSp'] + df_test['Parch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a9ab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['family_size'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d821e170",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['family_size'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8122f44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253073ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c339dd1",
   "metadata": {},
   "source": [
    "## EDA Continuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccee0ac",
   "metadata": {},
   "source": [
    "### Analyzing the Impact of Title Name on Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c04b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df_train, x='title_name', color='Survived', barmode='group', title='Survival by Title', text_auto=True,\n",
    "                  color_discrete_sequence=['#ff7f0e', '#3498db'],)\n",
    "\n",
    "fig.update_traces(marker=dict(line=dict(width=2, color='DarkSlateGrey')))\n",
    "\n",
    "# format the layout\n",
    "fig.update_layout(\n",
    "    xaxis=dict(showgrid=False, zeroline=False),\n",
    "    yaxis=dict(zeroline=False, gridcolor='white'),\n",
    "    paper_bgcolor='rgb(233,233,233)',\n",
    "    plot_bgcolor='rgb(233,233,233)',\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d7400",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- Men with the title \"Mr.\" had the most non-survivors. Women with titles \"Miss\" and \"Mrs.\" survived the most and had the fewest non-survivors. This shows that women were the top priority for survival during the incident."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d77878",
   "metadata": {},
   "source": [
    "### Analyzing the Impact of Family Size on Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7264917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df_train, x='family_size', y='Survived', palette='Set2')\n",
    "plt.title('Survival Rate by Embarkation Point')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148dc507",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "- Families with three or fewer members had a better chance of surviving. But families with four or more members had a lower chance of surviving. This shows that smaller families were safer during the incident."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b815338",
   "metadata": {},
   "source": [
    "<a class=\"anchor\"  id=\"4.2\"></a>\n",
    "## 4.2- Handling Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b4d933",
   "metadata": {},
   "source": [
    "### For train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d0d35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for missing values in data\n",
    "df_train.isna().sum() / df_train.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc155e2",
   "metadata": {},
   "source": [
    "> - Age has 20% missing data, which needs to be handled.\n",
    "> - Cabin has 77% missing data, a significant amount, and needs to be dropped\n",
    "> - Embarked has 0.22% missing data, which needs to be handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6364e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame based on the 'title_name' column\n",
    "df_train.sort_values(by='title_name', inplace=True)\n",
    "\n",
    "# Extract the 'Age' column as a 2D array for imputation\n",
    "age_data = df_train['Age'].values.reshape(-1, 1)\n",
    "\n",
    "# Initialize KNN imputer with k=5 (number of nearest neighbors)\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Perform imputation on the 'Age' column\n",
    "df_train['Age'] = imputer.fit_transform(age_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0b8dcb",
   "metadata": {},
   "source": [
    "> Sorting the titles helps us make better guesses about missing ages. When titles are sorted, people with similar titles are put together. Usually, people with the same title are of similar ages. So, when we fill in missing ages using the nearest neighbors (KNN imputer), we're using ages of people with the same or similar titles. This way, our guesses about missing ages become more accurate and realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3185a151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the 'Cabin' column due to its significant missing values of 77%.\n",
    "df_train = df_train.drop('Cabin', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcbee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SimpleImputer with most frequent strategy\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "\n",
    "# Reshape the column for imputation (required for 1D arrays)\n",
    "imputed_column = imputer.fit_transform(df_train['Embarked'].values.reshape(-1, 1))\n",
    "\n",
    "# Replace the original column with the imputed values\n",
    "df_train['Embarked'] = imputed_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5d2b53",
   "metadata": {},
   "source": [
    "> We will fill in the missing data of Embarked column using the SimpleImputer tool, which calculates the most frequent (mode) value and uses it to replace the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0932570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isna().sum() / df_train.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5826ee9b",
   "metadata": {},
   "source": [
    "> Now data is cleaned for train.csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029cad6d",
   "metadata": {},
   "source": [
    "### For test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e259a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isna().sum() / df_test.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844ef92b",
   "metadata": {},
   "source": [
    "> - Age has 20% missing data, which needs to be handled.\n",
    "> - Cabin has 78% missing data, a significant amount, and needs to be dropped\n",
    "> - Fare has 0.23% missing data, which needs to be handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34032e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame based on the 'title_name' column\n",
    "df_test.sort_values(by='title_name', inplace=True)\n",
    "\n",
    "# Extract the 'Age' column as a 2D array for imputation\n",
    "age_data = df_test['Age'].values.reshape(-1, 1)\n",
    "\n",
    "# Initialize KNN imputer with k=5 (number of nearest neighbors)\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Perform imputation on the 'Age' column\n",
    "df_test['Age'] = imputer.fit_transform(age_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0b860c",
   "metadata": {},
   "source": [
    "> Handling the age information in 'test.csv' the same way as we did in 'train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the 'Cabin' column due to its significant missing values of 78%.\n",
    "df_test = df_test.drop('Cabin', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81713ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SimpleImputer with mean strategy\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "\n",
    "# Reshape the column for imputation (required for 1D arrays)\n",
    "imputed_column = imputer.fit_transform(df_test['Fare'].values.reshape(-1, 1))\n",
    "\n",
    "# Replace the original column with the imputed values\n",
    "df_test['Fare'] = imputed_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f940f38e",
   "metadata": {},
   "source": [
    "> We will fill in the missing data of Fare column using the SimpleImputer tool, which calculates the median value and uses it to replace the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa8d1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isna().sum() / df_test.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fd0075",
   "metadata": {},
   "source": [
    "> Now data is cleaned for test.csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb5aa8",
   "metadata": {},
   "source": [
    "<a class=\"anchor\"  id=\"4.3\"></a>\n",
    "## 4.3- Handling Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39dc183",
   "metadata": {},
   "source": [
    "- **Nominal**: Categories without a meaningful order or ranking like (**Sex, title_name, Embarked**).\n",
    "- **Ordinal**: Categories with a meaningful order or ranking like **No ordinal features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014364bd",
   "metadata": {},
   "source": [
    "### for train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d0779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with Nominal Features with pandas `get_dummies` function.\n",
    "df_train = pd.get_dummies(df_train, columns=['Sex', 'title_name', 'Embarked'])\n",
    "\n",
    "encoded = list(df_train.columns)\n",
    "print(\"{} total features after one-hot encoding.\".format(len(encoded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121842dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75db1a28",
   "metadata": {},
   "source": [
    "### for test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed9bdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with Nominal Features with pandas `get_dummies` function.\n",
    "df_test = pd.get_dummies(df_test, columns=['Sex', 'title_name', 'Embarked'])\n",
    "\n",
    "encoded = list(df_test.columns)\n",
    "print(\"{} total features after one-hot encoding.\".format(len(encoded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d27caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb565d4",
   "metadata": {},
   "source": [
    "<a class=\"anchor\"  id=\"4.4\"></a>\n",
    "## 4.4- Data Split to Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43e17cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(['Survived'], axis=1)\n",
    "y_train = df_train['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ff6d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test.drop('PassengerId', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0087387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the results of the split\n",
    "print(\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print(\"Testing set has {} samples.\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8fe888",
   "metadata": {},
   "source": [
    "<a class=\"anchor\"  id=\"4.5\"></a>\n",
    "## 4.5- Handling Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9563f386",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f52efb",
   "metadata": {},
   "source": [
    "> - Data is imbalanced so we're using SMOTE to balance the data because under-sampling can cause data loss and affect prediction quality when the initial data is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a65116",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e32251",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd60d41",
   "metadata": {},
   "source": [
    "> - Now the Data is balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01cfda9",
   "metadata": {},
   "source": [
    "<a class=\"anchor\"  id=\"4.6\"></a>\n",
    "## 4.6- Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8abceeb",
   "metadata": {},
   "source": [
    "### Standardizing Continuous Features with StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796f0af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'family_size']\n",
    "\n",
    "# Creating a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fitting the StandardScaler on the training data\n",
    "scaler.fit(X_train[numerical_features])\n",
    "\n",
    "# Transforming (standardize) the continuous features in the training and testing data\n",
    "X_train_cont_scaled = scaler.transform(X_train[numerical_features])\n",
    "X_test_cont_scaled = scaler.transform(X_test[numerical_features])\n",
    "\n",
    "# Replacing the scaled continuous features in the original data\n",
    "X_train[numerical_features] = X_train_cont_scaled\n",
    "X_test[numerical_features] = X_test_cont_scaled\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e82b9",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9979849f",
   "metadata": {},
   "source": [
    "## Heatmap Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4457ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "sns.heatmap(df_train.corr(), annot=True,cmap='summer',fmt='.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb56043e",
   "metadata": {},
   "source": [
    "## Visualization of the Correlation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ef4a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "\n",
    "df_train.corr()['Survived'].sort_values(ascending = False).drop(['Survived']).plot(kind = 'bar', color = 'c')\n",
    "\n",
    "plt.xlabel('Feature', fontsize = 20)\n",
    "\n",
    "plt.ylabel('Target', fontsize = 20)\n",
    "\n",
    "plt.title('Correlation', fontsize = 20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d63c11",
   "metadata": {},
   "source": [
    "--------------------------------------------------------\n",
    "<a class=\"anchor\"  id=\"5\"></a>\n",
    "# 5- Models Training and Evaluation ‚öôÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091b252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of classifiers to evaluate\n",
    "classifiers = [\n",
    "    (\"Logistic Regression\", LogisticRegression(random_state=42, max_iter= 1500, n_jobs=-1)),\n",
    "    (\"KNN\", KNeighborsClassifier(n_neighbors=5, n_jobs=-1)),\n",
    "    (\"Gaussian Naive Bayes\", GaussianNB()),\n",
    "    (\"SVC\", SVC(random_state=42, probability=True)),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier(random_state=42)),\n",
    "    (\"Random Forest\", RandomForestClassifier(random_state=42, n_jobs =-1)),\n",
    "    (\"AdaBoost\", AdaBoostClassifier(random_state=42)),\n",
    "    (\"Gradient Boosting\", GradientBoostingClassifier(random_state=42)),\n",
    "    (\"LightGBM\", lgb.LGBMClassifier(random_state=42, verbose=-1)),\n",
    "    (\"XGBoost\", xgb.XGBClassifier(random_state=42, n_jobs =-1))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17896801",
   "metadata": {},
   "source": [
    "<a class=\"anchor\"  id=\"5.1\"></a>\n",
    "## 5.1- K-fold Cross-Validation Evaluation and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1236e61e",
   "metadata": {},
   "source": [
    "> Applying cross-validation through pipelines helps us thoroughly test machine learning models. It checks their performance across various data sets, ensuring a strong evaluation. By integrating feature selection within this process through pipelines, we carefully choose the best features. This method involves testing these features on different data parts, guaranteeing they work well across different situations. This meticulous approach ensures our selected features are reliable and effective, leading to a robust and widely applicable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ce5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RFE feature selector\n",
    "RFE_selector = RFE(lgb.LGBMClassifier(random_state=42, verbose=-1), n_features_to_select=12)\n",
    "\n",
    "\n",
    "# Creating lists for classifier names, mean_test_accuracy_scores, and results.\n",
    "results = []\n",
    "mean_test_accuracy_scores = []\n",
    "classifier_names = []\n",
    "\n",
    "for model_name, model in classifiers:\n",
    "    # Print model name\n",
    "    print(f\"For {model_name}:\")\n",
    "    \n",
    "    # Steps Creation\n",
    "    steps = list()\n",
    "    \n",
    "    steps.append(('feature_selector', RFE_selector))  # RFE feature selection\n",
    "    \n",
    "    steps.append((model_name, model))\n",
    "\n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "                        \n",
    "    # 5-fold Stratified Cross-Validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Perform cross-validation with train scores\n",
    "    cv_results = cross_validate(pipeline, X_train, y_train, cv=cv, scoring='accuracy',n_jobs=-1, return_train_score=True)\n",
    "    \n",
    "    print(f\"Cross-validation completed successfully for {model_name}\")\n",
    "    print('*' * 50)\n",
    "\n",
    "    # Append results to the list\n",
    "    results.append({\n",
    "        \"Model Name\": model_name,\n",
    "        \"Mean Train Accuracy\": np.mean(cv_results['train_score']),\n",
    "        \"Mean Test Accuracy\": np.mean(cv_results['test_score'])\n",
    "    })\n",
    "    \n",
    "    mean_test_accuracy_scores.append(np.mean(cv_results['test_score']))\n",
    "    classifier_names.append(model_name)\n",
    "\n",
    "# Create a DataFrame from the results list\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e743c4e",
   "metadata": {},
   "source": [
    "### Mean Test Accuracy Scores by Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b47ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame from the data\n",
    "data = pd.DataFrame({'Classifier': classifier_names, 'Test Accuracy': mean_test_accuracy_scores})\n",
    "\n",
    "# Creating Plotly bar chart\n",
    "fig = px.bar(data, x='Test Accuracy', y='Classifier', orientation='h', color='Test Accuracy',\n",
    "             title='Mean Test Accuracy Scores by Classifiers', text='Test Accuracy', color_continuous_scale='viridis')\n",
    "\n",
    "# Customizing the layout\n",
    "fig.update_layout(\n",
    "    xaxis_title='Test Accuracy',\n",
    "    yaxis_title='Classifier',\n",
    "    xaxis=dict(range=[0, 1]),\n",
    "    yaxis=dict(categoryorder='total ascending'),\n",
    "    showlegend=False,\n",
    "    height=500,\n",
    "    width=900\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d9901",
   "metadata": {},
   "source": [
    "> Among the various models evaluated during cross-validation, LightGBM Classifier emerged as the top performer. It exhibited exceptional performance with a Excellent Mean Train Accuracy score and Excellent Mean Test Accuracy score Notably, the model demonstrated no signs of overfitting, making it our chosen model for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49be653",
   "metadata": {},
   "source": [
    "## Selected Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c77cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RFE feature selector and fit it to the training data\n",
    "RFE_selector = RFE(lgb.LGBMClassifier(random_state=42, verbose=-1), n_features_to_select=12).fit(X_train, y_train)\n",
    "\n",
    "# Get selected feature names and importance values\n",
    "selected_feature_names = X_train.columns[RFE_selector.support_]\n",
    "feature_importances = RFE_selector.estimator_.feature_importances_\n",
    "\n",
    "# Create a DataFrame and sort features by importance\n",
    "feature_importance_df = pd.DataFrame({'Feature': selected_feature_names, 'Importance': feature_importances}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the top 12 selected feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df[:12])  \n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 12 Feature Importance (Selected by RFE)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa49fb94",
   "metadata": {},
   "source": [
    "--------------------------------------------------------\n",
    "<a class=\"anchor\"  id=\"6\"></a>\n",
    "# 6- Hyperparameter Tuning  üõ†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70f063d",
   "metadata": {},
   "source": [
    "> Hyperparameter tuning with GridSearch is crucial for optimizing model accuracy, preventing overfitting, and ensuring stable, robust predictions. It saves time, enhances computational efficiency, and leads to better-informed decisions, making it indispensable in machine learning model development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d32296",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for LightGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d6279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'LightGBM__learning_rate': [0.1, 0.2, 0.3],\n",
    "    'LightGBM__n_estimators': [50, 100, 200],\n",
    "    'LightGBM__num_leaves': [11 ,21 ,31],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288ea5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=[]\n",
    "steps.append(('feature_selector', RFE_selector))\n",
    "steps.append((\"LightGBM\", lgb.LGBMClassifier(random_state=42, verbose=-1)))\n",
    "pipeline=Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440fd4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GridSearchCV instance\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "# Fit the pipeline with GridSearch to the data\n",
    "grid_search.fit(X=X_train, y=y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500e54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean test score and mean train score for the best estimator\n",
    "mean_test_score = grid_search.cv_results_['mean_test_score'][grid_search.best_index_]\n",
    "mean_train_score = grid_search.cv_results_['mean_train_score'][grid_search.best_index_]\n",
    "\n",
    "print(\"Mean Train Score:\", mean_train_score)\n",
    "print(\"Mean Test Score:\", mean_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce872e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model=grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36755176",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253c49c6",
   "metadata": {},
   "source": [
    "<a class=\"anchor\"  id=\"6.1\"></a>\n",
    "## 6.1- ROC Curve for Final Model (LightGBM Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3197d67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for the positive class using the final model\n",
    "y_probabilities = final_model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Calculate the ROC curve and AUC score\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_probabilities)\n",
    "auc = roc_auc_score(y_train, y_probabilities)\n",
    "\n",
    "# Plotting the ROC curve\n",
    "sns.set(style='whitegrid')\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for LightGBM Classifier')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be433371",
   "metadata": {},
   "source": [
    "> An ROC curve with AUC = 0.99 means a excellent classifier. For the LightGBM  Classifier, it signifies the model correctly ranks 99% of the positive class higher than the negative class, based on the probability scores predicted by the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4fe996",
   "metadata": {},
   "source": [
    "<a class=\"anchor\"  id=\"7\"></a>\n",
    "# 7- Submissionüí°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on the test dataset using the final trained model\n",
    "predictions = final_model.predict(\n",
    "    df_test.drop('PassengerId', axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e706e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(\n",
    "    {\n",
    "     'PassengerId': df_test.PassengerId,\n",
    "     'Survived': predictions\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ebda2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849023fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
